\chapter{Time-to-Event Prediction with Neural Networks}
\label{survival-analysis}

% marginnote {{{
\marginnote{%
    \setlength{\parindent}{0pt}
    \vskip 1em
    What is survival analysis?
    \begin{description}[leftmargin=!, labelwidth=3em]
        \item[outcome] time until an event occurs.
            Can be measured in seconds, days, months, etc.
        \item[event] death, relapse, remission, engine failure, etc.
    \end{description}
    
    \begin{center}
    \begin{tikzpicture}
        \noindent
        \node (a) at (2.5, 0) {event};
        \node (b) at (0, 0) {start (\(t_0\))} edge ["time", ->] (a);
    \end{tikzpicture}
    \end{center}
}
% }}}

In the previous chapter, 
I gave an overview of machine learning and neural networks,
highlighting key ideas and concepts related to the studies in this thesis.
Neural networks specifically, 
were used in \nameref{chap:paper-2} and \nameref{chap:paper-3} to develop
prediction models for ischemic heart disease.
These models, however, diverge from classical neural network methods.
Instead, they include modifications that make them applicable
in modelling and analysis of time-to-event data.
This chapter will provide an introduction to the fundamentals
of survival analysis and will cover the
theory that enables the implementation of such analyses with
neural network models.
The chapter concludes with a discussion on different 
validation methods for time-to-event prediction models.

\section{What is Survival Analysis?}

Generally, 
survival analysis is the collection of statistical methods
for the modelling and analysis of time-to-event data,
which is data where the outcome variable of interest 
is the time until \enquote{something} happens.~%
~\autocite{kleinbaumSurvival2011}
This \enquote{something} is a particular event of interest,
which, depending on the type of analytical problem, 
could be cancer relapse, 
diabetes remission,
or death.
In cardiovascular research, 
common examples of time-to-event outcomes include
\begin{enumerate*}
    \item time to death due to any cause (all-cause mortality)
    \item time to death due to a specific cause,
        e.g. sudden cardiac arrest
    \item time to first occurence of a \ac{MACE}
\end{enumerate*}.
To figure out what processes and characteristics 
that are associated with such events, 
in survival analysis, we try to model the relationship between
explanatory variables and the number of weeks, months, or years 
until that particular event is likely to occur. 

% marginnote{{{
\marginnote{%
    Survival analysis have applications outside biomedical research.
    In engineering, it is called \textit{reliability analysis} and
    is used to model the time-to-failure of system-critical components 
    such as e.g. bearings or valves.
}% }}}

Although this task can be daunting in its own right, 
an additional complication to survival analysis 
is the presence of observations that are subject to 
censoring.
This concept, censoring, refers to cases 
where the event of interest has not been observed 
before the end of follow-up, 
e.g. when a study or experiment has to be stopped.
In such cases, 
we would know that a given subject did not experience a relapse 
in the three months he or she was included in the study, 
but after the study period ends, 
we have no information on the status of the patient. 
Including and utilizing this partial information
is a cornerstone in many survival analysis problems.

There exists different forms of censoring,
such as right censoring, left censoring, and interval censoring.
In the study designs used throughout this thesis 
we have only had to deal with right censoring,
the most common form of censoring,
so the two other types will not be described further.
See instead the text book by \citeauthor{kleinSurvival2003} 
for more details on this.
~\autocite{kleinSurvival2003}

\section{Fundamentals of Survival Analysis}

In survival analysis, 
the central outcome variable is survival time,
a non-negative random variable denoted as \(T\). 
When refering to specific values of \(T\), 
a lower case \(t\) is typically used.  
A survival dataset \(\mathfrak{D}\) of size \(n\) is given by
\begin{equation}
    \mathfrak{D}_n = \{(t_i, \sigma_i, \bm{x}_i) \mid i = 1, \ldots, n\} 
\end{equation}
where \(t_i = \min(T_{i}, C_i) \) is the survival time 
for the \(i\)th subject,
with \(T_i\) denoting the survival time
and \(C_i\) denoting the censoring time. 
Also, \(\bm{x}_i = (x_1, x_2, \dots, x_p)'\) is the covariate vector
and \(\sigma_i\) is the event indicator, which is defined as
\begin{equation}
    \label{eq:sigma-def}
    \sigma_i =
        \begin{cases}
            0 & \text{if subject is censored} \; (T_i >    C_i) \\
            1 & \text{if event is observed} \; (T_i \leq C_i)
        \end{cases}
\end{equation}

In the following, I will initially be assuming that \(T\) is 
continuous and that there is an absence of competing risks, 
however both of these assumptions will later be relaxed in the discussion 
of competing risks and discrete-time survival analysis.

\subsection{Basic Survival Quantities}
\label{sub:survival-quantities}

% figure: theoretical survival function{{{
\begin{marginfigure}%
	\begin{tikzpicture}[scale=2]
	  \draw[->] (0, 0) --  (2,0) 
		node[pos=0.0, below] {$0$}
		node[pos=0.5, below] {$t$}
		node[pos=1.0, below] {$\infty$};
	  \draw[->] (0, 0) --  (0,1) 
		node[pos=1.0, left] {$1$}
		node[pos=0.5, left] {$S(t)$};
	  \draw[-, color=color2, thick] 
		(0.05, 0.95) .. controls (1, 1) and (1, 0) .. (1.90, 0.05);
	\end{tikzpicture}
    \caption[A Theoretical Survival Function]{%
        An illustration of a theoretical survival function \(S(t)\). 
        Per definition, a survival function starts at \(S(0) = 1\)
        and is monotonically non-increasing.
        [\cite{kleinSurvival2003}]
    }
    \label{fig:survival-function}
\end{marginfigure}%}}}

In survival analysis, 
the central function of interest is 
the survival function \(S(t)\), 
that represents the probability 
of an individual still being alive after 
some specified duration of time, we have that 
%
\begin{equation}
    S(t) = \PR (T > t), \quad 0 < t < \infty.
\end{equation}

The survival function is 
the integral of the probability density function, \(f(t)\),
and is the complement to the cumulative distribution function, \(F(t)\),
which means that
~\autocite{kleinSurvival2003}
%
\begin{equation}
    S(t) = 1 - F(t) 
    \quad \text{and} \quad 
    S(t) = \int_{t}^{\infty} f(u) \, \diff u
\end{equation}

Another fundamental quantity is the hazard function, or hazard rate,
which represents the instantaneous failure rate at a given timepoint,
and is defined as
%
\begin{equation}
    \label{eq:hazard-function}
    \lambda(t) = \lim_{\Delta t \to 0} 
        \frac{\PR (t \leq T < t + \Delta t \mid T \geq t)}{\Delta t}
\end{equation}
% 
from which it can be shown that
~\autocite{kleinSurvival2003}
%
\begin{equation}
    \lambda(t) = \frac{f(t)}{S(t)} = -\frac{\diff}{\diff t} \ln[S(t)].
\end{equation}
%
and thus the hazard function completely describes the distribution of \(T\),
such that all the the other quantities can be obtained from it---%
as well as the other way around.

In terms of its interpretation, 
from \cref{eq:hazard-function} it follows that \(\lambda(t)\Delta t\) 
is a measure of the conditional probability of failure in a small time
window, given that the individual is still alive at time \(t\).
~\autocite{kleinSurvival2003}

Analogous to the relation between \(f(t)\) and \(F(t)\), 
integrating \(\lambda(t)\) with resport to \(t\),
we obtain cumulative hazard function, defined as
%
\begin{equation}
    \Lambda(t) = \int_{0}^{t} \lambda(u) \, \diff u = -\ln[S(t)].
\end{equation}

\subsection{The Kaplan-Meier Estimator}

The survival function of a population,
can be estimated using the Kaplan-Meier method,
which is the standard estimator of the survival function.
~\autocite{kaplan1958nonparametric}
~\autocite{kleinSurvival2003}
In this approach, 
the distinct failure times are ordered such that%
\footnotemark
\begin{equation*}
    t_{(1)} < t_{(2)} < \ldots < t_{(j)},
\end{equation*}
%
and we introduce two quantities to keep track of
the number of failures \(\widebar{D}(j)\), 
as well as the number of subjects still at risk \(\widebar{A}(j)\).
They are defined as
\begin{equation}
\begin{aligned}
    \bar{D}(j) &= \card \{i \in \{1, \dots, n\} \mid t_i = t_{(j)}, \sigma_i = 1\} \\
    \bar{A}(j) &= \card \{i \in \{1, \dots, n\} \mid t_i > t_{(j)}\},
\end{aligned}
\end{equation}
and the Kaplan-Meier estimator can then be formulated as 
\begin{equation}
    \widehat{S}(t)
    =   \prod_{j \mid t_{(j)} \leq t} 
        \frac{
            \bar{A}(j) -
            \bar{D}(j)
        }{
            \bar{A}(j)
        }
    =   \prod_{j \mid t_{(j)} \leq t} 
        1 - \frac{
            \bar{D}(j)
        }{
            \bar{A}(j)
        }.
\end{equation}

\footnotetext{%
    Following the example of [\cite{kleinbaumSurvival2011}], 
    the \(t\)'s denoted with subscripts within parentheses \(t_{(j)}\)
    refers to the \(j\)th element of the ordered distinct failure times and
    are thus different from \(t_1, t_2, \ldots, t_i\) that refers to the 
    observed failure time of subject \(1\), \(2\), and \(i\)
}

The Kaplan-Meier estimator is thus as step-function that decreases
after each observed event.
While the Kaplan-Meier estimator is useful 
for summarising the survival of a population, 
it does not account for the effect of covariates.
Instead, another approach is needed for regression analyses.

\subsection{Cox's Proportional Hazards Model}

To describe and model the relationship between explanatory variables
and time-to-event phenomenons, a widely used statistical model is 
the Cox proportional hazards model. 
~\autocite{coxRegression1972}
This model seeks to model the hazard function over time \(t\),
of an individual with a covariate vector \(\bm{x} = (x_1, x_2, \dots)'\),
and assumes that it takes the form of
%
\begin{equation}
    \label{eq:cox}
    \widehat{\lambda} (t \,|\, \bm{x}) = \hzt \exp [g(\bm{x})],
\end{equation}
%
where \(\hzt\)
is an unspecified baseline hazard function,
and \(g(\bm{x})\) is some parametric function.
For this reason, the Cox model 
is referred to as a semi-parametric model.
In its classical formulation, 
this function is a linear combination of parameters 
\(\bm{\beta}\) and covariates \(\bm{x}\),
as given by

\begin{equation}
    g(\bm{x}) 
    = \bm{\beta}' \bm{x} 
    = \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p
\end{equation}

In estimation of the parameters \(\bm{\beta}\),
the baseline hazard \(\hzt\) is treated as a nuisance function
and the coefficients are estimated by
maximising a partial likelihood
in which \(\hzt\) has been abstracted away.
~\autocite{kalbfleischStatistical2002}

A central assumption in the Cox model, 
at least in the standard version with fixed covariates
(\(\bm{\beta}\) instead of \(\bm{\beta}(t)\)),
is that of proportional hazards.
Let \(\bm{x}\) and \(\bm{x}'\) be two different 
covariate vetors, now the ratio between their
respective Cox-estimated hazards is

\begin{equation}
    \label{eq:hazard-ratio}
    \begin{aligned}
    \frac%
        {\widehat{\lambda}(t \,|\, \bm{x} \hfill)}%
        {\widehat{\lambda}(t \,|\, \bm{x'})}
    &=
    \frac%
        {\hzt \exp (\bm{\beta}\cdot\bm{x}\hfill)}%
        {\hzt \exp (\bm{\beta}\cdot\bm{x'})} \\
    &=
    \frac%
        {\exp (\bm{\beta}\cdot\bm{x}\hfill)}%
        {\exp (\bm{\beta}\cdot\bm{x'})} \\
    &= \exp (\bm{\beta} \cdot (\bm{x} - \bm{x'}))
    \end{aligned}
\end{equation}

Since the right-hand side of the equation does not include a term for \(t\),
the hazard ratio between the two samples are constant and 
they are thus proportional to one another., 
This shows that by assuming the hazard takes the form of \cref{eq:cox},
then it is also assumed that the hazards between two subjects are proportional.
Although this assumption is a strong one, 
and the validity of the Cox model relies on it, 
the assumption makes interpretation of parameters easier.
~\autocite{tutzModeling2016}
For example, 
in an randomized clinical trial
studying the survival effect of a new type of medication, 
we can let \(x = 1\) represent the experimental treatment  
and \(x' = 0\) represent standard of care, 
then the hazard ratio in \cref{eq:hazard-ratio} takes the form of
%
\begin{equation}
      \exp \left(\beta (x - x')\right)
    = \exp \left(\beta (1 - 0)\right)
    = \exp (\beta ),
\end{equation}
%
which means that if \(\beta < 0\), 
then the hazard of the experimental treatment is 
\(\exp({\beta})\) times lower than standard of care
and should therefore be preferred.%
\sidenote{% 
This example is a slightly modified version of the 
one given in \cite[pp. 50]{tutzModeling2016}}


\section{Analysis of Competing Risks}

\begin{marginfigure}[3em]% {{{
    \tikzstyle{outcome}=[%
        rectangle, rounded corners, minimum height=5mm, fill=color3
    ]
    \centering
    \begin{tikzpicture}[x=0.60\linewidth, y=1cm]
    \graph [edge quotes={font=\scriptsize, fill=white}, 
            nodes      ={draw, outcome, sloped, minimum width=1cm}]{
        alive [fill=color4] -> dead [> "\(\lambda(t)\)" ];
    };
    \end{tikzpicture}
    \caption[A Single State Model]{
        A simple survival analysis setup 
        involves modelling a single transition between states 
        \enquote{alive} and \enquote{dead}.
    }
    \label{fig:ssm}
\end{marginfigure}% }}}

Up to this point, the description of concepts in survival analysis has
assumed the presence of only a single event type, such as all-cause mortality
(\cref{fig:ssm}).
In practice, particularly in clinical settings, 
this single-event model can be too restrictive,
and instead one needs to consider competing risks
(\cref{fig:msm}).
By definition, a competing risk is a secondary event whose occurence 
prevents the primary event from occuring.
For example,
in a study where the primary outcome is cardiovascular mortality,
deaths from non-cardiovascular causes are a competing risk.

In the following, 
we let \(R \in \{1, \dots, k\}\) denote the \(k\) different competing risks,
and then slightly update the definition of the event indicator \(\sigma_i\)
(\cref{eq:sigma-def}), such that
\begin{equation}
    \sigma_i =
        \begin{cases}
            0 & \text{if subject is censored} \; (T_i >    C_i) \\
            r & \text{if cause r is observed} \; (T_i \leq C_i, R = r).
        \end{cases}
\end{equation}

\begin{marginfigure}% {{{
    \tikzstyle{outcome}=[%
        rectangle, rounded corners, minimum height=5mm, fill=color3
    ]
    \centering
    \begin{tikzpicture}[x=0.60\linewidth, y=0.85cm]
    \graph [edge quotes={font=\scriptsize, fill=white}, 
            nodes      ={draw, outcome, sloped, minimum width=1cm}]{
        alive [fill=color4] -> {
            cause 1 [> "\(\lambda_1(t)\)" ],
            cause 2 [> "\(\lambda_2(t)\)" ],
            cause k [> "\(\lambda_k(t)\)" ],
        };
    };
    \end{tikzpicture}
    \caption[A Multi-State Model]{
        A survival analysis setup with competing risks
        involves modelling transitions between states 
        \enquote{alive} and \(k\) different absorbing
        states, \enquote{cause 1} to \enquote{cause \(k\)}
    }
    \label{fig:msm}
\end{marginfigure}
% }}}

\subsection{Cause-Specific Survival Quantities}

To describe time-to-event phenomena with competing risks, 
we introduce the cause-specific hazard function and 
cumulative-incidence function.
With \(R \in \{1, \dots, k\}\) denoting the \(k\) different competing risks, 
the cause-specific hazard function is defined as
\begin{equation}
    \lambda_r(t) = \lim_{\Delta t \to 0} 
        \frac{\PR (t \leq T < t + \Delta t, R=r \mid T \geq t)}{\Delta t}
\end{equation}
where \(r\) refers to a specific value of \(R\).
The cause-specific cumulative incidence function is defined as
~\autocite{kalbfleischStatistical2002}
\begin{equation}
    F_r(t) = \PR(T \leq t, R = r).
\end{equation}

The overall hazard and cumulative incidence, 
which combines failures of any of the \(k\) causes,
correspond to the hazard function 
and the cumulative distribution function 
in the single-event setting, that is
\begin{equation}
    \lambda(t) = \sum_{r=1}^{k} \lambda_r(t)
    \quad \text{and} \quad
    F(t) = \sum_{r=1}^{k} F_r(t).
\end{equation}

\subsection{The Aalen-Johansen Estimator}

In the competing risk setting, 
the methodology previously presented
have to be adjusted accordingly.
For example, 
simply treating competing events as censored 
and applying the standard Kaplan-Meier estimator, 
would lead to a biased estimate of \(F(t)\).
~\autocite{pepeKaplan1993}
Instead, an alternative approach is the Aalen-Johansen estimator
that allows estimation of the cause-specific cumulative incidence.
~\autocite{aalenEmpirical1978}
Of note, the Aalen-Johansen is a general method for estimating
transition probabilities in state-transition models,
and can be used to describe complex multi-state models,
including those with repeated events and with non-terminal states.
~\autocite{survival-package}
However, we will be assuming a standard competing-risk setting
with \(k\) different terminal states, 
as depicted in \cref{fig:msm}.
 
If we again order the distinct failure times, 
corresponding to any cause, 
such that
\(t_{(1)} < t_{(2)} < \ldots < t_{(j)}\),
and update the definition of \(\bar{D}(j)\) to keep track of cause-specific
events, such that we have
\begin{equation}
\begin{aligned}
    \bar{D}(j, r) &= \card \{i \in \{1, \dots, n\} \mid t_i = t_{(j)}, r_i = r\} \\
    \bar{A}(j)    &= \card \{i \in \{1, \dots, n\} \mid t_i > t_{(j)}\}.
\end{aligned}
\end{equation}
Now, the Aalen-Johansen estimator of the cumulative incidence function
can be defined as 

\begin{equation}
    \widehat{F}_r(t)
    =   \sum_{j \mid t_{(j)} \leq t}{
        \!\!
        \widehat{S}(t_{(j-1)})
        \frac{\bar{D}(j, r)}{\bar{A}(j)}
    }
\end{equation}

\section{Time-to-Event Prediction}

Up until now, 
I have outlined various concepts foundational to survival analysis,
focusing primarily on quantities and statistics of time-to-event outcomes
at a popoulation level.
These measures play an important role in understanding 
and interpretation of survival data.

In the context of precision medicine, however,
the emphasis shifts towards making individualized predictions
taking distinct patient-level characteristics into account.
Consequently, as described in \cref{chap:ml-and-nn}, 
the primary concern lies in 
making accurate predictions on unseen data,
rather than in the exploration of disease etiology and underlying mechanisms.

For prediction of time-to-event outcomes, classical approaches 
include models based on the previously presented semi-parametric Cox model 
as well as various parametric survival models, 
such as those based on exponential, Weibull, or log-normal distributions.
~\autocite{kleinSurvival2003}
This thesis, however, 
explores the use of contemporary machine learning methods 
in time-to-event prediction,
with a particular emphasis on the application of neural networks.

\subsection{Neural Networks and Time-to-Event Outcomes}

The first application of neural networks for time-to-event prediction
was demonstrated by
\citeauthor{faraggiNeural1995} in
\citeyear{faraggiNeural1995},
and involves parameterising the parametric part of the Cox model
with a neural network, 
such that the \(g(\bm{x})\) term in \cref{eq:cox} is a 
flexible neural network model instead of a simple linear function.
\autocite{faraggiNeural1995}

\vspace{1em}
\begin{equation*}
    \widehat{\lambda} (t \,|\, \bm{x}) = \hzt \exp [
    \eqnmarkbox[color2]{node1}{
        g(\bm{x})
    }]
\end{equation*}
\annotate[yshift=.6em]{left}{node1}{use neural network}

This approach was later further refined
in the \emph{DeepSurv} paper from 
\citeyear{katzmanDeepSurv2018a},
in which modern neural network techniques
were added to Faraggi-Simon framework, 
which markedly improved its usefulness.
~\autocite{katzmanDeepSurv2018a}
\citeauthor{katzmanDeepSurv2018a} showed that the flexibility 
offered by neural networks led to increased performance
in both synthetic and real-life time-to-event prediction applications
compared to a standard Cox model.
~\autocite{katzmanDeepSurv2018a}







\vspace{10em}





Another tudy found that a comprehensive machine learning model, incorporating
clinical and CT data, was superior in predicting 10-year cardiovascular and
coronary heart disease deaths compared to traditional assessments. This model
showed higher accuracy, with area under the curve values of 0.845 for
cardiovascular death and 0.860 for coronary heart disease death. It
outperformed both the coronary artery calcium score and the atherosclerotic
cardiovascular disease risk score.
~\autocite{nakanishiMachine2021}




In an international collaboration involving \num{11011} patients, \citeauthor{thanMachine2019} developed a machine learning model 
designed for objective assessment of myocardial infarction likelihood 
in patients suspected of having this condition. 
Their model demonstrated superior discrimination and calibration 
compared to existing rule-out algorithms.
~\autocite{thanMachine2019}






\subsection{Types of Approaches}

DeepSurv.
A modern implementation of the Faraggi-Simon network.
parameterises the hazard.
single risk
compares to cph,
~\autocite{katzmanDeepSurv2018a}


DeepHit
parameterises the probability density function
~\autocite{leeDeepHit2018}



The N-MTLR approach, 
an neural network version of MTLR,
~\autocite{yuLearning2011}
parameterises the survival function.
~\autocite{fotsoDeep2018}



DRSA

RNN-SURV

N-MTLR

Dynamic-DeepHit

Survtrace

Deep-CSA







\subsection{Cox-based}

\subsection{Parameterising the Hazard}


\section{Discrete-Time Survival Analysis}









\section{Discrete-Time Survival Analysis}

Most textbooks on survival analysis treats survival time as continuous, 
and that is also usually the case across the biomedical litterature.
However, handling time as a something discrete might be advantegous.
In practice, most measurements of time is inherently discrete 
with durations being recorded in, for example, days; months; and weeks.
The continuous time approaches presented so far, 
are applicable on discrete time data,
however, methods that are designed specifically for discrete time-to-event 
data have some advantages~\autocite{tutzModeling2016}:

\begin{itemize}
    \item If observed event times are inherently discrete, 
        then modelling them as such is arguably more appropriate. 
    \item In the discrete-time setting, hazards can be formulated as 
        conditional probabilities which are much more intuitive to 
        both interpret and understand.
    \item Discrete time-to-event models are more easily transferred to 
        other more general purpose modelling frameworks 
        such as generalized linear models, random survival forests, 
        neural networks.
\end{itemize}

In \nameref{chap:paper-1} and \nameref{chap:paper-2}, 
we take advantage of the latter,
in constructing neural network-based survival models
for precision prognostication in ischemic heart disease.
The following section gives a brief outline of
central concepts in discrete survival analysis. 

\subsection{Definitions}

In a discrete-time framework, 
continuous follow-up time is divided into into \(q\) contiguous intervals
%
\begin{equation*}
	[0, a_1), [a_1, a_2), ..., [a_{q-1}, a_q), [a_{q}, \infty)
\end{equation*}
%
and the time to event variable \(T\) is denoted 
with a positive integer \(t = 1, 2, ...\),
which points to the interval \([a_{t-1}, a_{t})\).
With this discrete time scale,
the cause-specific hazard function \(\lambda_{r}(t)\) is 
the conditional probability of an individual experiencing 
the event in that specific interval,
given that the individual is still alive at the start of the interval
%
\begin{equation}
    \lambda_{r}(t \,|\, \bm{x}) = \PR (T = t, R = r \mid T \geq t, \bm{x})
\end{equation}
%
given a vector \(\bm{x}\) of predictor variables. 







See kvamme for a review of the different methods




\vspace{10em}

We 

The DeepSurv model estimates the hazard function \(h(t|\mathbf{x})\).

\begin{equation}
    h(t | \mathbf{x}) = h_0(t) \exp(f(x, \theta))
\end{equation}

\begin{itemize}
    \item Random survival trees and forest
    \item Penalized or boosted Cox regression
    \item Neural network-based extensions
\end{itemize}




\citeauthor{gensheimerScalable2019} 
parameterized the hazard using a neural network,
and showed that their method provide well calibrated
model estimates with high model discrimination.

It is an approach with the same idea as the one presented by 
\citeauthor{brownUse1997} in 1997 \autocite{brownUse1997}.


DeepHit parameterizes the probability mass function 
of the survival distribution.
They also include a ranking loss to improve model discrimination.
\citeauthor{kvammeContinuous2021} showed that the DeepHit method
has excellent discrimination but is poorly calibrated.


\section{Model formulation}

Let \(C_i\) be the right-censoring time 
and \(T_i\) be the event time for for individual \(i\).
A discrete time survival dataset \(\mathfrak{D}\) is a set of \(n\) tuples
\((t_{i}, \delta_{i}, \mathbf{x}_{i})\)
where \(t_i = \min\{T_i, C_i\}\) is the event time,
\(\delta_{i} \in \{0, ..., m\}\) is the event indicator 
(with \(\delta_i = 0\) defined as censoring),
and \(\mathbf{x}_{i} \in \mathbb{R}^d\) 
is a \(d\)-dimensional vector of time-independent predictors or covariates.

The proposed model parametrizes the cause-specific conditional hazards,
and the output \(\Lambda\) is then a 
\(n \times q \times (m + 1)\) matrix of probabilities
where values across the last dimension all sum up to 1.

Without competing risks,
the negative log-likelihood of the model output is
%
\begin{equation}
    \mathrm{NLL}(\Lambda) =
	- \sum_{i=1}^{n}
	\sum_{t=1}^{q}
    y_{is} \log [\ \lambda(s|\mathbf{x}_i) ]\
    + (1 - y_{is}) \log [\ 1 - \lambda(s|\mathbf{x}_i) ]\
\end{equation}
%
which is the loss-function used in \citeauthor{gensheimerScalable2019}.


In the case of several competing target events,
it is necessary to define several hazard functions,
one for each distinct outcome type.
Let \(R \in \{1, ..., m\}\) denote the competing risks.
Then, for disrete time \(T \in \{1, ..., q\}\), 
the cause-specific hazard function from cause $r$ is defined by
%
\begin{equation*}
    \lambda_{r}(t | \mathbf{x}) = \Pr (T = t, R = r | T \geq t, \mathbf{x})
\end{equation*}
%
where \(\mathbf{x}\) is a vector of time-independent predictor variables. 
The different hazard functions, 
\(\lambda_{1}(t|\mathbf{x}), ..., \lambda_{m}(t|\mathbf{x})\) 
can be combined into the overall hazard function defined as
%
\begin{equation}
    \lambda(t|\mathbf{x}) 
    = \sum_{r=1}^{m}(\lambda_{r}(t|\mathbf{x}))
    = \Pr(T = t | t \leq T, \mathbf{x})
\end{equation}
%
From this it follows that the survival function is
%
\begin{equation}
    S(t|\mathbf{x})
    = \Pr(T >= t|\mathbf{x})
    = \prod_{i=1}^{t}(1 - \lambda(i|\mathbf{x}))
\end{equation}

The model uses $m + 1$ categorical responses.
The responses are either any of the $m$ competings risks, 
or conditional survival.
%
\begin{equation}
    \Pr(T > t | T \geq t, \mathbf{x})
    = 1 - \sum_{r = 1}^{m} \lambda_r (t | \mathbf{x})
    = 1 - \lambda (t | \mathbf{x})
\end{equation}

\section{Model formulation}

Continuous follow-up time is divided into into $q$ intervals
%
\begin{equation*}
	[0, a_1), [a_1, a_2), ..., [a_{q-1}, a_q), [a_{q}, \infty)
\end{equation*}
%
and the time to event variable \(T\) is denoted 
with a positive integer \(t = 1, 2, ...\),
which points to the interval \([a_{t-1}, a_{t})\).
With this discrete time scale,
the cause specific hazard function \(\lambda_{r}(t)\) is a conditional probability defined as
%
\begin{equation*}
    \lambda_{r}(t | \mathbf{x}) = \Pr (T = t, R = r | T \geq t, \mathbf{x})
\end{equation*}
%
given a vector \(\mathbf{x}\) of predictor variables.

The discrete hazard function is defined as
the probability of failure in the interval \(t\),
given that the individual is still alive at the end of the preceding interval
\(t - 1\).

In a time-to-event model with several competing target events,
we describe the process with multiple hazard functions, 
that each describe a specific target outcome or risk.
Typically, 
we refer to the individual hazard functions as cause-specific hazards.

For discrete time \(T \in \{1, ..., s+1\}\), and with a given vector \(\mathbf{x}\) of time-independent covariates,
the cause-specific hazard from cause \(r\) is defined as

\begin{equation}
    \lambda_{r}(t|\mathbf{x}) 
    = P(T = t, R = r | t \leq T, \mathbf{x})
\end{equation}

Which represents the probability of 

The overall hazard function is

\begin{equation}
    \lambda(t|\mathbf{x}) 
    = \sum_{r=1}^{m}(\lambda_{r}(t|\mathbf{x}))
    = P(T = t | t \leq T, \mathbf{x})
\end{equation}

The survival function is the unconditional probability of an event in period \(t\) given the specific covariates is 

\begin{equation}
    S(t|\mathbf{x})
    = P(T >= t|\mathbf{x})
    = \prod_{i=1}^{t}(1 - \lambda(i|\mathbf{x}))
\end{equation}

From the hazard we can obtain the survival function 
%
\begin{equation*}
    S(t | \mathbf{x}) 
    = \Pr (T \geq t | \mathbf{x}) 
    = \prod_{s = 1}^{t-1} (1 - \lambda(s | \mathbf{x}))
\end{equation*}
%
which represents the unconditional probability of surviving interval \(t\) given the specific covariates cf. \autocite{tutzModeling2016}.

Of note, this interpretation is considerably more accessible than the usual
continuous hazard one.


\begin{equation}
    S(\tau) = P(T > \tau) = \sum_{i = 1}^{\tau - 1} (P(T = \tau - i))    
\end{equation}

The cause specific cumulative incidence function is

\begin{equation}
	CIF_r(t|\mathbf{x})
	= \sum_{i=1}^{t}\lambda_{r}(t|\mathbf{x}) S(t-1|\mathbf{x})
\end{equation}

Let the data be given by \(t_{i}, r_{i}, \delta_{i}, \mathbf{x}_{i}\),
where \(r_{i} \in \{1, ..., m\}\) indicates the target event. 
Assuming random censoring at the end of the interval
with \(t_i = \min\{T_i, C_i\}\), 
where events are defined by the indicator function

\begin{equation}
\delta_i = 
	\begin{cases}
		1, & T_i \leq C_i \\
		0, & T_i > C_i
	\end{cases}
\end{equation}

\begin{subequations}
\begin{equation}
	nll =
	- \sum_{i=1}^{n}
	\sum_{t=1}^{ti}
	\left(
		\sum_{r \in C} \delta_{itr} \log(\lambda_{r} (t | \mathbf{x}))
		+ \delta_{it0} \log\left(
			1 - \sum_{r \in C} \lambda_{r} (t | \mathbf{x})
		\right)
	\right)
\end{equation}

Or alternatively 

\begin{equation}
	nll =
	- \sum_{t=1}^{q}
	\sum_{i \in R_s}
	\delta_{is} \log(s|\mathbf{x}_i) 
	+ (1 - \delta_{is}) (1 - \log(s|\mathbf{x}_i)))
\end{equation}
\end{subequations}

In our approach, the neural network parametrises the logistic hazards.
The goal is to learn \( \hat{h}(t | \mathbf{x}) \)
for each of the competing events.




The discrete probability function

\begin{equation}
    f_l = \Pr (T \in A_l) = S(t_{l-1}) - S(t|l)
\end{equation}

the discrete hazard rate

\begin{equation}
    h_l = \Pr (T \in A_l | T > t_{l-1}) = \frac{f_l}{S(t_{l-1})}
\end{equation}

    
The contribution to the likelihood function of the \textit{i}th subject

For uncensored subjects, the contribution is
%
\begin{equation}
    \Pr (T_i \in A_{l_i}) = f_{l_i} = h_{l_i} \prod_{l=1}^{l_i-1}(1-h_{l_i})
\end{equation}
%
while for censored subject, the contribution is
%
\begin{equation}
    \Pr (T_i > t_{l_i}) = S(t_{l_i}) = \prod_{l=1}^{l}(1-h_{l_i})
\end{equation}

A similar approach have previously been described \autocite{biganzoliFeed1998}.




\section{Evaluating prediction models}

Visualization of model performance.
A calibration shows the link between 
expected outcome proportion 
and predicted probabilities.

\subsection{Calibration}

A model is said to be well-calibrated
if the probabilistic predictions are reliable.

With a reliable model, 
in a group of individuals predicted to develop diabetes
with a \SI{10}{\percent} risk, 
we would expect to exactly \SI{10}{\percent} to actually develop diabetes.


\subsection{Discrimination}

Discrimination refers to a models ability to rank individuals according 
to their actual risk of experiencing a given event.
The main measure for discrimination is the area under the receiver
operating characteristic curve (AUC).
For survival models, Harrel's C-index is typically used,
although it is actually an improper scoring metric.
Instead, the time-dependent AUC should be used instead.

%% DEEPSURV

In 2018, DeepSurv 

In the approach by Faraggi-Simon, they use a neural network to 
parameterise the log-risk function, using a slightly modified Cox partial 
likelihood as the optimization target.
~\autocite{faraggiNeural1995}
The same approach was used in the \enquote{DeepSurv} paper,
but instead paired with contemporary deep learning methods.
~\autocite{katzmanDeepSurv2018a}





In survival analysis of competing risks, 
different risks can be modelled marginally,
independently of one another.
This relies on the latent failure assumption.

However, in real-life, the different competing risks 
or \enquote{failure modes} can influence one another
by some shared mechanisms.
Therefore, marginal independent time-to-event models
can therefore be to simplistic.
Instead, it is in such cases
more reasonable to model the competing risks jointly,
such that the dependence structure shared between them
can be leveraged.


