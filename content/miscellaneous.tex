
\chapter{Misc}

For predictive models to be effective,
they must enable clinicians to guide treatment by:
%
\begin{enumerate*}
    \item providing actionable insights on modifiable risk factors
    \item being adapted to the individual patient
    \item being generally applicable in different populations
\end{enumerate*}

%------------------------------------------------------------------------
\vskip 5em

AI-driven medical prediction models are able to synthesize large amounts
of electronic health data into risk scores or estimates that can be used
to assess the risk and condition of the individual patient~\autocite{handelmanEDoctor2018}.

Complications also occur in otherwise asymptomatic patients, 
and for that reason risk evaluation should apply to both patients
with out without symptoms.
~\autocite{knuuti20192020}

Although revascularization can relieve symptoms for the culprit lesions
responsible for myocardial ischemia, it may not prevent the progression
of other neighboring lesions and future acute thrombotic events.
~\autocite{libbyPathophysiology2005}

\textquote[byrne20232023]{%
    A number of prognostic models \textelp{}
    have been formulated into clinical risk scores and, among
    these, the GRACE risk score offers the best discriminative performance and
    is therefore recommended for risk assessment.%
}


Myocardial tissue damage can be detected with high sensitivity by measuring 
the release troponins and creatinine-kinase from the sarcolemma.
~\autocite{falkPathogenesis2006}

The esc guidelines on risk-prevention highlights 
that one of the currents gaps in evidence,
is comparing the performance of competing risk-adjusted 
cardiovascular disease risk models versus the standard models.
~\autocite{visseren20212021}


Subgroup identification seeks to identify groups of individuals with an
increased treatment response, which
\citeauthor{kosorokPrecision2019} refers to as 
\textquote[kosorokPrecision2019]{%
    finding the right patient for the right treatment%
}.


Electronical health records are a rich source of health data,
and can be used to find connections between risk factors and diseases.

Precision medicine is prevention and treatment approaches

that takes individual variation into account.


The terms
\enquote{precision},
\enquote{personalized},
and \enquote{individualized medicine}
are often used interchangeably.

avoid toxicity and improve efficacy

\begin{minipage}{\linewidth}
    \emph{Notes:}
    Causality.
    Different strategies exists for guiding the development of 
    predicting therapy response.
    We might be able to use prognostic and diagnostic factors
    discovered in the previous track to predict treatment response.
    Example: HER is prognostic for survival in breast cancer,
        but is also predictive of treatment response against X.

\end{minipage}


As with the other tracks, track 3 gan generate further knowledge 
that in turn can feed back to the more general phenotyping of patients
\autocite{konigWhat2017}. 

An electronic health record
is a systematized collection of health data 
stored in a digital format.
Electronic health records may include data such as
medical history, drug prescriptions, laboratory test results,
radiology images, body measurements, and billing information.
EHRs enable following patients in both time and space;
not only in the physical space that is the cardiology department
and the intensive care unit, but also in the health-and-disease space.

\marginnote{
    The five Vs of big data:
    \begin{itemize}
        \item Volume
        \item Velocity
        \item Variety
        \item Value
        \item Veracity
    \end{itemize}
}


\section{Clinical decision support systems}

Challenges in developing machine learning models from electronic health records
includes syntactic interoperability, 
which specifies the format and structure of the data.
Many different interopability formats exists, 
but their adoption varies considerably.

Benefits of interopable digital health data
~\autocite{lehneWhy2019}.
Test~\cite{lehneWhy2019}.

\begin{itemize}
    \item Large-scale observational studies.
    \item Less time spend on data cleaning and pre-processing.
    \item Interoperable exchange format could further cross-institutional
    and international collaboration, which would make it easier 
    to reproduce and externally validate e.g. prognostic applications.
    In the case of rare diseases with very limited number of patients, 
    international pooling of data could enable analysis
    that otherwise would not be possible.
    \item Faster research and development process.
    \item If data is known to conform to a well-defined format,
    computer software can be written without explicit access to the data.
    This solve many issues related to sensitive data or
    data that are otherwise subject to strict data protection regulations.
\end{itemize}


Healthcare data are usually private and scattered across various applications,
which makes it difficult to share data and generate robust results 
that translate to different and diverse populations.

AI approaches rely on data that accurately represent
the real-life distribution of the underlying problem.
We can not exclusively rely on data that have been carefully curated 
from few and often very similar sources. 
In order to capture subtle relationships 
between health and disease patterns,
we need to include many and diverse cases~%
\autocite{riekeFuture2020}.

Automated diagnosis in ophthalmology 
from optical coherence tomography scans~\autocite{defauwClinically2018}.

\section{Computational models in cardiology}

Using single-lead lectrocardiogram recordings from \num{53877} patients,
Hannun et al.\autocite{hannunCardiologistlevel2019}
constructed a deep neural network
to identify 12 difference classes of cardiac rythm.
In the validation of their algorithm on an independent test set,
the authors were able to show that the computer model
significantly outperforms the average cardiologist. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Miscellaneous}

Systematic deugging and continuous monitoring and validation 
is of utmost importance if we are to release AI algorithms into the wild%
\autocite{topolHighperformance2019}.

In computer vision tasks in the medical domain,
deep-learning models have achieved physician-level performance
in many different diagnostic tasks
ranging from


Deep learning is at its core a form of representation learning.
~\autocite{estevaGuide2019}
Each layer in a neural network is a different representation,
and by stacking several of such layers on top of each others,
the representation in one hidden layer
feeds into the next layer and
is thereby being transformed into an even more abstract representation%
~\autocite{estevaGuide2019}.
This allow neural network models to identify patterns in sparse, noisy,
and highly heterogeneous data without the need for expert feature engineering,
which makes them particularly pertinent to healthcare applications.% 
~\sidecite[-2em]{norgeotCall2019}



\subsection{Scope of Explanations}

The scope of an explanation is the difference between
explanations for a complete model and
explanations for a single output.
Global explanation covers feature importance estimates 
for the entire dataset.
Local explanations, on the other hand, seeks to explain
the impact of the specific example under scrutiny.

A SHAP-waterfall plot is an example of a local explanation.
A saliency map of a chest radiograph that shows
which pixels contributed to the label \enquote{liver cancer}
is another example.

Shapley values measures the marginal contribution
of each individual feature.


One limitation of XAI models is the accuracy and relevance of explanations.
Explainability algorithms such as SHAP are only approximations
of the complete model.
In other words, the fidelity is not perfect and therefore neither
is the explanation.
However, for black-box models such as neural networks,
it is the next best thing.

Another tudy found that a comprehensive machine learning model, incorporating
clinical and CT data, was superior in predicting 10-year cardiovascular and
coronary heart disease deaths compared to traditional assessments. This model
showed higher accuracy, with area under the curve values of 0.845 for
cardiovascular death and 0.860 for coronary heart disease death. It
outperformed both the coronary artery calcium score and the atherosclerotic
cardiovascular disease risk score.
~\autocite{nakanishiMachine2021}

In an international collaboration involving \num{11011} patients, \citeauthor{thanMachine2019} developed a machine learning model 
designed for objective assessment of myocardial infarction likelihood 
in patients suspected of having this condition. 
Their model demonstrated superior discrimination and calibration 
compared to existing rule-out algorithms.
~\autocite{thanMachine2019}


\subsection{Discretization of Continuous Time}

In applying discretization to continuous time, 
we assume that both events and censorings occurs at 
\(a_t\)

%%%%%%

\begin{equation}
    S(t \giv \vec{x}) = \PR(\Tid > \tid \mid \vec{x}) = 
    \prod_{i=1}^{\tid} (1 - \lambda(i \giv \vec{x}))
\end{equation}

\begin{equation}
    \begin{aligned}
    f(\tid \giv \vec{x}) 
    &= \PR (\Tid = \tid \mid \vec{x}) \\
    &= \PR (\Tid = \tid \mid \Tid \geq \tid, \vec{x}) 
       \PR (\Tid  \geq \tid \mid \vec{x}) \\
    &= \PR (\Tid = \tid \mid \Tid \geq \tid, \vec{x}) 
       \PR (\Tid  > \tid - 1 \mid \vec{x}) \\
    &= \lambda (\tid \giv \vec{x}) \, S(\tau - 1 \giv \vec{x})
    \end{aligned}
\end{equation}

%%%%%%

\citeauthor{gensheimerScalable2019} 
parameterized the hazard using a neural network,
and showed that their method provide well calibrated
model estimates with high model discrimination.


DeepHit parameterizes the probability mass function 
of the survival distribution.
They also include a ranking loss to improve model discrimination.
\citeauthor{kvammeContinuous2021} showed that the DeepHit method
has excellent discrimination but is poorly calibrated.


Without competing risks,
the negative log-likelihood of the model output is
%
\begin{equation}
    \mathrm{NLL}(\Lambda) =
	- \sum_{i=1}^{n}
	\sum_{t=1}^{q}
    y_{is} \log [\ \lambda(s|\mathbf{x}_i) ]\
    + (1 - y_{is}) \log [\ 1 - \lambda(s|\mathbf{x}_i) ]\
\end{equation}
%
which is the loss-function used in \citeauthor{gensheimerScalable2019}.

\section{Evaluating prediction models}

Visualization of model performance.
A calibration shows the link between 
expected outcome proportion 
and predicted probabilities.

\subsection{Calibration}

A model is said to be well-calibrated
if the probabilistic predictions are reliable.

With a reliable model, 
in a group of individuals predicted to develop diabetes
with a \SI{10}{\percent} risk, 
we would expect to exactly \SI{10}{\percent} to actually develop diabetes.


\subsection{Discrimination}

Discrimination refers to a models ability to rank individuals according 
to their actual risk of experiencing a given event.
The main measure for discrimination is the area under the receiver
operating characteristic curve (AUC).
For survival models, Harrel's C-index is typically used,
although it is actually an improper scoring metric.
Instead, the time-dependent AUC should be used instead.

%% DEEPSURV

In 2018, DeepSurv 

In the approach by Faraggi-Simon, they use a neural network to 
parameterise the log-risk function, using a slightly modified Cox partial 
likelihood as the optimization target.
~\autocite{faraggiNeural1995}
The same approach was used in the \enquote{DeepSurv} paper,
but instead paired with contemporary deep learning methods.
~\autocite{katzmanDeepSurv2018a}


In survival analysis of competing risks, 
different risks can be modelled marginally,
independently of one another.
This relies on the latent failure assumption.

However, in real-life, the different competing risks 
or \enquote{failure modes} can influence one another
by some shared mechanisms.
Therefore, marginal independent time-to-event models
can therefore be to simplistic.
Instead, it is in such cases
more reasonable to model the competing risks jointly,
such that the dependence structure shared between them
can be leveraged.



%%

\textcite{mohammadDevelopment2022} developed a neural network model
for prediction of 1-year all-cause mortality and admission with heart failure 
after an incident myorcardial infarction.
The model was trained using data from the Swedish \acsfont{SWEDEHEART}
registry and externally validated using the Western Denmark
Heart Registry.
~\autocite{mohammadDevelopment2022}
The model showed great discrimination in both internal and external
validation cohorts.



Many different clustering algorithms exists, 
but the choice of using the \ac{MCL} algorithm was motivated by 
the following aspects. 
First, the \ac{MCL} algorithm is fast and has been explicitly designed 
to clustering analysis of very large networks with huge amounts 
of vertices and edges.
Secondly, the number of clusters neither can or should be specified in
advance, and instead the issue of \enquote{how many clusters} is
is not dealt with in an arbitrary manner, but
rather by a strong internal logic.
~\autocite{vandongenGraph2008}
The granularity, or coarseness, of the clustering is instead controlled 
by the so-called inflation parameter.


Specifying the number of clusters is neither needed nor possible, 
instead the issue of \enquote{how many clusters} is handled by the
algorithm itself.



In this study, we used the broadest possible definition of comorbidites,
and considered 

\subsection{Defining Comorbidities}

For instance, the definition of multimorbidity used in 
\textcite{formanMultimorbidity2018} is 
\textquote{two or more medical diseases or conditions, 
each lasting more than one year}.

Other studies have limited their definition 
to cover only 20 common chronic conditions.
\autocite{roccaPrevalence2014}

We defined comorbidity as the historical co-occurence
of different diagnosis codes in the individual patient record.

% The sizes, average age, and hazard ratios are aditionally reported 
% in \cref{tab:cluster-results}.
% \begin{table}[htbp]% →
% \centering
% \footnotesize
% \sisetup{
%     detect-all
% }
% \setlength{\tabcolsep}{2pt}
% \begin{tabular}{
%   l
%   S[table-format=4.0]
%   S[table-format=2.1]
%   S[parse-numbers=false]
%   S[table-format=1.3]
%   S[table-format=1.3]
% }
%     %\multirow{2}[3]{*}{Size} & 
% \toprule
% {} & {\multirow{2}{*}{Size}} &
% \multicolumn{2}{c}{Age} & 
% \multicolumn{1}{c}{New Ischemic Events} & 
% \multicolumn{1}{c}{Non-\ac{IHD} mortality} \\
% \cmidrule(lr){3-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6}
% %%%%
% {}  & {} & { \(\mu\)} & {(SD)} & {HR} & {HR}  \\
% \midrule
% C1  & 7191  & 64.8 & (11.3)  & 1.000 & 0.856 \\
% C2  & 5990  & 58.6 & (11.5)  & 0.825 & 0.600 \\
% C3  & 4641  & 56.8 & (11.4)  & 0.757 & 0.586 \\
% C4  & 4401  & 69.6 & (10.2)  & 0.920 & 1.461 \\
% C5  & 4290  & 63.9 & (10.7)  & 1.402 & 1.629 \\ [0.8ex]
% C6  & 3589  & 59.7 & (10.9)  & 0.969 & 0.675 \\
% C7  & 3309  & 63.8 & (11.0)  & 0.889 & 0.611 \\
% C8  & 2802  & 71.1 & (10.9)  & 0.943 & 0.842 \\
% C9  & 2581  & 63.7 & (11.8)  & 1.314 & 1.789 \\
% C10 & 2562  & 74.2 & (9.6)   & 0.978 & 0.928 \\ [0.8ex]
% C11 & 2292  & 66.1 & (11.0)  & 0.926 & 0.650 \\
% C12 & 2213  & 70.3 & (10.2)  & 0.920 & 0.805 \\
% C13 & 2070  & 58.6 & (10.2)  & 0.946 & 0.577 \\
% C14 & 2070  & 68.2 & (9.6)   & 1.146 & 3.390 \\
% C15 & 2040  & 63.9 & (10.1)  & 1.031 & 0.784 \\ [0.8ex]
% C16 & 1654  & 64.1 & (12.1)  & 1.107 & 1.761 \\
% C17 & 1281  & 65.3 & (9.9)   & 1.001 & 1.761 \\
% C18 & 1251  & 68.2 & (9.8)   & 1.790 & 3.421 \\
% C19 & 1168  & 58.5 & (9.7)   & 0.752 & 1.571 \\
% C20 & 1119  & 71.5 & (11.3)  & 1.213 & 1.782 \\ [0.8ex]
% C21 & 1000  & 61.0 & (11.0)  & 1.116 & 0.890 \\
% C22 & 988   & 69.2 & (10.4)  & 1.023 & 0.978 \\
% C23 & 935   & 58.7 & (12.2)  & 1.609 & 2.275 \\
% C24 & 932   & 67.9 & (10.1)  & 0.787 & 1.589 \\
% C25 & 860   & 56.2 & (9.9)   & 0.978 & 2.691 \\ [0.8ex]
% C26 & 852   & 58.7 & (12.1)  & 0.939 & 1.108 \\
% C27 & 823   & 65.1 & (10.9)  & 1.201 & 1.289 \\
% C28 & 686   & 71.7 & (8.0)   & 0.866 & 1.786 \\
% C29 & 550   & 57.2 & (11.1)  & 0.906 & 0.985 \\
% C30 & 533   & 61.2 & (11.7)  & 1.874 & 5.364 \\ [0.8ex]
% C31 & 520   & 64.4 & (11.2)  & 1.052 & 1.484 \\
% \bottomrule
% \end{tabular}
% \caption{Clinical characteristics of the study population by cluster}
% \label{tab:cluster-results}
% \end{table}% ←


\textcite{popescuArrhythmic2022}
used deep learning methods for predicting
survival in patients at high risk of cardiac death
from cardiac magnetic resonance imaging.



Cardiovascular diseases are among the most common conditions in 
patients with multimorbidity.
~\autocite{dunlayMultimorbidity2016}
