\chapter{Principal Findings, Limitations, and Future Perspectives}
\label{chap:findings-and-limitations}

In this thesis, I have explored the use of informatics-based approaches 
for addressing critical aspects pertinent to the understanding and management 
of \ac{IHD}. 
Central to this research was the application of advanced \ac{ML} methods
on large-scale electronic health data for development of precision medicine
approaches for secondary prevention in \ac{IHD}.
This involved identifying and characterizing patterns of multimorbidity 
in \ac{IHD} and developing feature-rich clinical prediction models for 
precision prognostication.

In this chapter, I will briefly reiterate the main findings of the 
included studies, adress some general and study-specific limitations
of the work undertaken, and discuss perspectives for future research.

\section{Principal Findings}

Throughout the previous chapters, 
I have described three different scientific manuscripts
detailing our research in the framework of \ac{ML}-based precision medicine.
The following provides a brief summary of the principal findings 
of each of the included studies.

\subsection{Comorbidity Clustering in Ischemic Heart Disease}

In \studyi{}, 
we used unsupervised clustering analysis to explore the comorbidity landscape
of \num{72249} patients with \ac{IHD}. 
We used the broadest possible definition of multimorbidity and 
defined comorbidity as the historical co-occurence of a broad
array of diagnosis codes in the individual patient records.
The accrued patient-specific comorbidity profiles,
containing more than \num{3000} different diagnosis codes,
led to the identification of 31 distinct patient subgroups.
These clusters represent distinct patterns of multimorbidity 
linked to \ac{IHD}, were found to be associated with 
specific risk of subsequent outcomes,
and can be used to better understand the complex
nature of multimorbidity in \ac{IHD}.

\subsection{Time-to-Event Prediction of All-Cause Mortality}

In \studyii{}, 
we presented the development and validation of 
a novel neural network-based prognostication model
for prediction of all-cause mortality in 
patients with \ac{IHD}.
This model, \pmhnet{1}, utilises a discrete-time approach 
for modelling of time-to-event data with neural networks and
can provide time-specific probability estimates of survival
across a five-year prediction horizon.
The model was developed using a large and diverse dataset 
\num{39746} \ac{IHD} patients from the \ac{EDHR}
and incorporates a comprehensive set of 584 features,
including diagnosis history, procedural codes, laboratory test results,
and clinical measurements obtained from \ac{EHR} data and registry data.
Compared to both the \acs{GRACE} 2.0 score, 
and a neural network-based model limited to the \acs{GRACE} features,
the feature-rich \pmhnet{1} model provided a significant improvement
in model performance.
External validation on an independent Icelandic dataset of \num{8287} patients 
further showed that the model performance is generalizable.
Furthermore, by including \acs{SHAP}-analysis we were able to provide
explanations of the model output and assess feature importance.
The study established \pmhnet{1} as a valuable tool for post-angiography 
assessment of all-cause mortality risk in \ac{IHD} patients,
and can potentially aid clinicians in making informed decisions 
about treatment and management of \ac{IHD}.

\subsection{Time-to-Event Prediction with Competing Risks}

In \studyiii{}, 
we introduced an new framework for construction of neural network-based
competing risk models and presented the development \pmhnet{2}, 
an advanced iteration of our \ac{IHD} prognostication algorithm.
The updated \pmhnet{2} model provide cause- and time-specific 
risk estimates for all-cause mortality, cardiovascular mortality, 
cardiovascular complications, and new myorcardial ischemia events.
From internal validation, we found the model estimates to
be well-calibrated and to accurately predict patient at 
both high and low risk of the four different outcomes.
Compared to the standard practice of treating competing events as 
censored, we showed that models capable of jointly modelling 
competing risks were associcated with a better model discrimination
and calibration.
While still a work in progress, the presented work establishes
the usefullness of the updated methodology and presents
\pmhnet{2} as a promising tool for prognostication in 
\ac{IHD}.

\section{Limitations}

Despite their strengths,
a number of limitations and constraints
related to the presented studies,
potentially affects the overall interpretation of the findings.
In the following,
I will address and discuss both study-specific
and general limitations of our research.

\subsection{Definition of Comorbidities}
\label{sec:comorbidities}

In \studyi{},
a possible limitation relates to its exclusively data-driven definition
of multimorbidity that included the historical co-occurence of a very broad 
array of diagnosis codes.
This approach constrasts with that of similar studies in the domain.
As an example, 
\textcite{formanMultimorbidity2018}
defined multimorbidity as
\textquote{two or more medical diseases or conditions, 
each lasting more than one year}. 
Similarly, another study also limited their definition
to only cover chronic conditions, specifically the 20 most
common ones.
\autocite{roccaPrevalence2014}
Unlike these studies that focused on chronic conditions,
our study did not differentiate between chronic and acute diagnoses. 
As a result, our clustering could, for example, 
be influenced by a 3-year old pneumonia diagnosis.
However, since we accrued the number of admissions for each diagnosis
the chronic nature of certain conditions is likely implicitly accounted.

\subsection{Lack of Temporal Resolution in Features}

In this thesis, a notable limitation is the absence of temporal resolution in
the input features, affecting both the clustering in \studyi{} and the
prediction models in \studyii{} and \studyiii{}. This lack of temporal
granularity means that the models and analyses do not account for the timing
and sequence of medical events or diagnoses.

In \studyi{}, the clustering could have been enhanced by somehow 
incorporating the chronological order of the diagnoses in the patient vectors.
This would allow for a more nuanced description of the comorbidity burden
of the individual patient, and could in addition help alleviate the limitation
of chronic versus acute conditions described in \cref{sec:comorbidities}.
Previous research within our group by \textcite{jensenTemporal2014} illustrated
a method to identify temporal disease trajectories from retrospective registry
data. They also demonstrated the use of these trajectories in clustering
applications. However, this method only captures temporal patterns with clear
directionality, which could exclude many of the comorbidities we considered in
our study. Thus, while it offers a possible avenue for future research,
it also has its limitations in fully representing the range of
comorbidities.

In \studyii{} and \studyiii{}, time resolved input features could enable 
the neural network models to learn from sequential patterns of medical
events and diagnoses. Such information could provide valuable information
for accurate prognostication. For instance, knowing the progression of 
\ac{IHD} and comorbidities could potentially inform more timely and tailored
interventions. Additionally, the study design used in the development of
\pmhnet{1} and \pmhnet{2} was limited in scope to only provide predictions
subsequent to an index coronary angiography. While these models 
might be applicable at other timepoints, it is not something that we have tested,
and it would probably affect their performance.

Alternatives to address this limitation include the use of time-series data 
and longitudinal study designs such as those based on landmark analysis.
\autocite{dafniLandmark2011}
These approaches can facilitate the creation of dynamic risk prediction
models.
\autocite{vanhouwelingenDynamic2007}
In the context of neural networks, this would likely involve
using architectures like 
\ac{LSTM}~\autocite{hochreiterLong1997}
or Transformers~\autocite{vaswaniAttention2017},
which are designed to use sequential features.

\subsection{Generalisability of Clusterings}

For \studyi{},
an inherent limitation of clustering applications is 
the lack of standardized techniques for external \enquote{validation}
compared to those in supervised learning.
In supervised learning, 
evaluating the model generalizability is straightforward:
apply the model to a test set, 
which could be an internal hold-out set or an external dataset,
and then directly measure its performance.
However, this approach is not feasible in most unsupervised clustering
applications due to the absence of predefined labels.
Alternative strategies do exists,
as outlined by \textcite{ullmannValidation2022},
and includes:
\begin{itemize}
    \item Applying the clustering algorithm to a representative external
        dataset. Subsequently, examine if the cluster structure obtained on
        this external dataset shares internal and external characteristics with
        the original clustering. 
    \item Transferring the original clustering to the external dataset by first
        using, for example, a supervised classifier. 
        ~\autocite{ullmannValidation2022}
        This classifier is trained to predict
        the cluster labels derived from the original dataset and then applied
        to the external dataset. 
        If clustering on the external data is consistent with the transferred
        labels, then it indicates that the cluster algorithm have 
        captured patterns that are not just specific to the initial
        dataset.
\end{itemize}

Such approaches, while not direct validations in the traditional sense, 
could provide insights into the overall generalisability of the clustering 
outside the context of the original dataset.
~\autocite{ullmannValidation2022}
Nonetheless, these approaches have not been implemented in our research.
Consequently, we do not assert that the clustering presented is definitively
the \enquote{best} but rather utilize it as a method to condense the extensive
array of diagnostic codes into interpretable subgroups.

\subsection{Choice of Clustering Algorithm}

A further potential limitation of \studyi{} 
is that we did not compare or test other 
clustering methodologies besides the \ac{MCL} algorithm.
Although numerous different clustering algorithms exists,
our choice of using \ac{MCL} was motivated by a number of key aspects.
Firstly, the \ac{MCL} algorithm is fast and has been explicitly designed 
to handle very large networks with a substantial number of vertices
and edges.
~\autocite{vandongenGraph2008}
Secondly, in this algorithm,
the number of clusters neither can nor should be pre-specified.
Instead the issue of \enquote{how many clusters} is handled
by a strong internal logic, rather than being dealt with in an
arbitrary manner as is common in other clustering algorithms.
~\autocite{vandongenGraph2008}

\subsection{Lack of Primary Care Data}

A fundamental limitation in our research stems from the nature of the data
accessed, as all our studies primarily utilized hospital data. This
reliance on hospital data is likely to lead to an underrepresentation 
of data related to conditions and diseases primarily managed in primary
care settings, including hypertension, non-complex infections, and various
soft-tissue disorders.
~\autocite{finleyWhat2018} 

In \studyiii{}, we attempted to mitigate this limitation by incorporating
prescription data, which can serve as proxy for the conditions managed
in primary care.
However, it is important to note that prescription data is only partly
able to compensate for the lack of detailed primary care patient records.

\subsection{Limitations of Explainable AI}

The last limitation I would like to higlight are some general shortcomings
of \ac{XAI} that often are overlooked. Currently, \ac{XAI} is only implemented
in \studyii{}, but our plan is include it in \studyiii{} as well, and for 
this future work, these limitations also apply.

As described earlier in this thesis, 
the goal of \ac{ML} is to make accurate predictions on unseen data,
and as consequence, the \enquote{how} and \enquote{why} of predictions
is of less concern.
However, for critical applications, including healthcare, it is 
generally agreed that transparency is important and that 
the \enquote{black box} nature of \ac{ML} needs to be 
adressed. 
~\autocite{topolHighperformance2019}
This is exemplifed by article 15,
of the European Union's \ac{GDPR},
~\autocite{EuropeanParliament2016a}
which specifies an requirement for transparency that 
applies to algorithmic decision-making.%
\sidenote{%→
    Item (h) in paragraph 1 of the \acs{GDPR} article 15 states that
    \textquote{%
     the existence of automated decision-making, including profiling, referred to
     in Article 22(1) and (4) and, at least in those cases, meaningful information
     about the logic involved, as well as the significance and the envisaged
     consequences of such processing for the data subject.}%
}% ←

For \ac{ML}, including neural networks, \ac{XAI} is a form of post-hoc
analysis that seeks to provide the required transparency for 
otherwise complex and non-transparent models.
In this domain, \acsu{SHAP}-analysis~\autocite{lundbergUnified2017},
which we utilized in \studyii{} 
for providing explanations of the \pmhnet{1} model,
is arguably one of the most popular \ac{XAI} approaches.
\ac{SHAP},
along with other similar approaches,
relies on the usage of simpler surrogate model
to estimate the expected marginal contribution
of each feature to the model's output.
~\autocite{bellePrinciples2021}
However, this approach requires certain assumptions,
including the premise that the model can be locally
approximated by a simpler model and that features
are independent.
~\autocite{lundbergUnified2017}
The independence assumptions is very strong and often very unrealistic,
which is likely to bias the estimates of feature contributions---%
nevertheless, this approach is still in widespread use.
Recent research has demonstrated that is is possible to partially mitigate
this limitation, but at the expense of a significantly increased computational
complexity, which can limit its practical usefulness.
\autocite{aasExplaining2021}

\ac{XAI} algorithms such as \ac{SHAP} are approximations 
of the complete model, therefore the fidelity is not perfect
and as a consequence, neither are the explanations.
Currently, there are no established standards for assessing the quality
of these explanations.
While it is possible the esimate the error of the approximations,
this does not necessarily indicate whether the explanations are interpretable
and understandable to end-users. 

From a practical standpoint,
the explanations offered by \ac{XAI} can be subject to misinterpretation,
particularly by users less familiar with technical details of 
the \acsu{AI}-model and the \ac{XAI} method used.
During the clinical implementation of the \pmhnet{1} model for a, 
currently ongoing, clinical trial~\autocite{bundgaardClinical2023}, 
we provide \ac{SHAP} values alongside the model predictions to inform 
clinicians on the basis of the model predictions.
However, a pilot experiment in which clinicians were asked to qualitatively 
evaluate the model's output and explanations revealed some challenges in 
the interpretation of these.

For example, one clinician found it counterintuitive that the model identified
hyperlipidemia (\acs{ICD-10}: E78) as a factor contributing to increased 
survival.
While it is possible that this finding is caused by the aforementioned
limitations, there are other plausible explanations as to why
hyperlipidemia could be identified as a \enquote{protective} feature.
It is important to note that \ac{SHAP} values are correlations and 
do not imply causation.
Patients already known with hyperlipidemia prior to
their index coronary angiography may represent a 
group of patients with non-acute manifestations of \ac{IHD},
which relative to the median \ac{IHD} patient could
have improved survival.
Additionally, these patients are likely to have
initiated statin treatment before the time of prediction,
which once again, could be associated with a improved
prognosis relative to the median patient.

This example underscores the complexities inherent in
interpreting \ac{XAI} explanations, especially when they appear
counterintuitive or misaligned with conventional medical knowledge. 
It also emphasizes the need for thorough education and effective communication
with healthcare professionals regarding clinical decision support tools that
incorporate these technologies.

\section{Future Perspectives}

Adressing the various just discussed limitations and constraints
all represent topics for future research, some more important than others.
To conclude this thesis, I would like to highlight two central challenges
related to this thesis project of utmost importance for future data-driven 
research in precision medicine.

\subsection{Clinical Implementation of Machine Learning Models}

In this thesis, I have argued for the potential benefits of \acsfont{AI/ML}
methodologies in improving the clinical treatment and management of patients
with \ac{IHD}. We have developed two neural network-based \ac{IHD}
prognostication models, evaluated their performance using state-of-the-art
statistical metrics, and concluded that high-dimensional \ac{ML}-based
models are superior to existing alternatives.
However, the theoretical clinical impact is, as of now, only just that---%
theoretical. 

It is generally accepted that
the overwhelming majority of published medical prediction models 
are never implemented in clinical practice.
~\autocite{steyerbergPrognosis2013}
To increase the adoption of \ac{ML}-based prognostic models, 
we need more prospective clinical studies to ascertain 
the impact and practical applicability of such models.
This includes exploring how \acsfont{AI/ML} can positively   
affect clinical decision-making, patient outcomes, and 
allocation of healthcare resources.
Such research is crucial in realising the potential of 
\acsfont{AI/ML} in the advancement of precision medicine.

As an extension of the work presented in \studyii{},
although not included as a central part of this thesis,
we have established a collaboration with the public company 
\acsfont{CIMT} to implement \pmhnet{1} in \enquote{Sundhedsplatformen}, 
the \ac{EHR} system used in the Capital Region and Region Zealand hospitals,
for prospective clinical validation.
We successfully integrated the model into a real-world clinical \ac{EHR}
setting and have initiated a clinical trial.
~\autocite{bundgaardClinical2023} This study is still in progress, but
independently of its outcomes, the mere implementation of the model in a
clinical setting is a significant achievement.

Looking ahead, depending on the findings of this trial,
the focus needs to shift towards the implementation of processes
for continuous monitoring of model performance, for regularly updating
the model with new data, for refining models to include additional 
endpoints (\pmhnet{2}), and several other important aspects.

\subsection{Sharing of Healthcare Data}

As outlined in \cref{chap:data-foundation}, 
the studies presented in this thesis draws on hospital data from 
more than \num{2.6} million individuals, which originates from
combination of different data sources, including
electronic health records, national registries, 
and clinical quality databases.
In the context of this thesis project,
a major challenge have been
processing, combinining, cleaning, and 
organizing these diverse sources of data
into curated datasets appropriate for 
\ac{ML} applications.

However, because such data, for good reason, is subject to ethical 
and privacy-protecting rules and regulations, it cannot realistically 
be shared with researchers outside our institution.
This means that it is impossible for others to reproduce our findings,
develop and benchmark rival models, and benefit from the data cleaning
and curation that have already been done.
It is a waste of resources and limits the development of the field as a whole.

It is evident that 
there exists an unmet need for regulations and approaches that enable 
combining and benefitting from otherwise siloed datasets.
In this context,
the concept of federated health data networks have been 
suggested as a possible solution to overcome existing barriers preventing 
sharing of data.
~\autocite{hallockFederated2021}
In the ongoing effort of establishing a European Health Data Space, 
the European Commission's proposal have also included procedures and
regulations for secondary research use of health data.
As suggested by \textcite{raabFederated2023}, 
this effort could be coupled with the establishment of
pan-European federated health data network, which could break
the many barriers limiting current big data-based clinical research.

Future research should focus on technical solutions for 
establishing such networks, development of algorithms for 
distributed machine learning, and construction of interoperability 
formats which could further cross-institutional and international 
collaboration.
